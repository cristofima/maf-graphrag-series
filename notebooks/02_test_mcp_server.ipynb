{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "0",
   "metadata": {},
   "source": [
    "# GraphRAG MCP Server Testing\n",
    "\n",
    "This notebook tests the GraphRAG MCP tools directly and via the HTTP server.\n",
    "\n",
    "**Prerequisites:**\n",
    "1. Knowledge graph indexed: `poetry run python -m core.index`\n",
    "2. Azure OpenAI configured in `.env`\n",
    "\n",
    "**Optional (for HTTP tests):**\n",
    "\n",
    "3. MCP server running: `poetry run python run_mcp_server.py`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "import os\n",
    "from pathlib import Path\n",
    "from dotenv import load_dotenv\n",
    "\n",
    "# Add project root to path\n",
    "sys.path.insert(0, str(Path.cwd().parent))\n",
    "\n",
    "# Load environment variables\n",
    "load_dotenv()\n",
    "\n",
    "# Verify configuration\n",
    "print(\"Configuration Check:\")\n",
    "print(f\"   Azure OpenAI Endpoint: {os.getenv('AZURE_OPENAI_ENDPOINT')}\")\n",
    "print(f\"   Chat Deployment: {os.getenv('AZURE_OPENAI_CHAT_DEPLOYMENT', 'gpt-4o')}\")\n",
    "print(f\"   Embedding Deployment: {os.getenv('AZURE_OPENAI_EMBEDDING_DEPLOYMENT', 'text-embedding-3-small')}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2",
   "metadata": {},
   "outputs": [],
   "source": [
    "import logging\n",
    "\n",
    "# ---------------------------------------------------------------------------\n",
    "# Suppress noisy INFO logs from LiteLLM and GraphRAG internals.\n",
    "#\n",
    "# LiteLLM (used internally by GraphRAG) logs every Azure OpenAI API call at\n",
    "# INFO level. Global search triggers 20+ parallel LLM calls, producing 40+\n",
    "# log lines per query. GraphRAG also attaches a RichHandler that emits styled\n",
    "# HTML fragments captured by Jupyter as text/html outputs, inflating the\n",
    "# .ipynb file size without adding user value.\n",
    "#\n",
    "# We keep WARNING level so that actionable messages (e.g. token-limit\n",
    "# warnings from mixed_context.py) are still visible for traceability.\n",
    "# ---------------------------------------------------------------------------\n",
    "\n",
    "logging.basicConfig(level=logging.WARNING)\n",
    "\n",
    "# LiteLLM — main noise source (INFO on every completion call)\n",
    "logging.getLogger(\"litellm\").setLevel(logging.WARNING)\n",
    "\n",
    "# GraphRAG internals — keeps token-limit WARNINGs, removes routine INFO\n",
    "logging.getLogger(\"graphrag\").setLevel(logging.WARNING)\n",
    "\n",
    "# httpx / httpcore — suppress HTTP-level request chatter\n",
    "logging.getLogger(\"httpx\").setLevel(logging.WARNING)\n",
    "logging.getLogger(\"httpcore\").setLevel(logging.WARNING)\n",
    "\n",
    "# OpenAI SDK — suppress retry/connection INFO\n",
    "logging.getLogger(\"openai\").setLevel(logging.WARNING)\n",
    "\n",
    "print(\"\\u2705 Logging configured \\u2014 noisy INFO logs suppressed (WARNING+ preserved)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3",
   "metadata": {},
   "source": [
    "## 1. Test MCP Server Connection (optional)\n",
    "\n",
    "Only needed if you started the MCP server with `poetry run python run_mcp_server.py`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4",
   "metadata": {},
   "outputs": [],
   "source": [
    "import httpx\n",
    "\n",
    "# Test MCP server health\n",
    "async def test_mcp_server():\n",
    "    try:\n",
    "        async with httpx.AsyncClient() as client:\n",
    "            response = await client.get(\"http://localhost:8011/health\", timeout=5.0)\n",
    "            print(f\"✅ MCP Server Status: {response.status_code}\")\n",
    "            print(f\"   Response: {response.text}\")\n",
    "    except httpx.ConnectError:\n",
    "        print(\"❌ MCP Server not running!\")\n",
    "        print(\"   Start it with: poetry run python run_mcp_server.py\")\n",
    "    except Exception as e:\n",
    "        print(f\"❌ Error: {e}\")\n",
    "\n",
    "await test_mcp_server()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5",
   "metadata": {},
   "source": [
    "## 2. Test Entity Query Tool\n",
    "\n",
    "List and search entities directly (no server needed)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6",
   "metadata": {},
   "outputs": [],
   "source": [
    "from mcp_server.tools.entity_query import entity_query_tool\n",
    "\n",
    "# List entities by type\n",
    "result = await entity_query_tool(entity_type=\"person\", limit=5)\n",
    "\n",
    "if \"error\" in result:\n",
    "    print(f\"Error: {result['error']}\")\n",
    "else:\n",
    "    print(f\"Found {result['total_found']} person entities (showing {result['returned']}):\\n\")\n",
    "    for entity in result[\"entities\"]:\n",
    "        print(f\"  - {entity['name']} ({entity['type']})\")\n",
    "\n",
    "# Search by name\n",
    "print(f\"\\nSearching for 'Emily Harrison':\")\n",
    "result = await entity_query_tool(entity_name=\"Emily Harrison\")\n",
    "if \"error\" not in result:\n",
    "    for entity in result[\"entities\"]:\n",
    "        print(f\"  - {entity['name']}: {entity['description'][:100]}...\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7",
   "metadata": {},
   "source": [
    "## 3. Test Local Search (Entity-Focused)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8",
   "metadata": {},
   "outputs": [],
   "source": [
    "from mcp_server.tools.local_search import local_search_tool\n",
    "\n",
    "queries = [\n",
    "    \"Who leads Project Alpha?\",\n",
    "    \"Who resolved the GraphRAG index corruption incident?\",\n",
    "    \"What technologies are used in Project Beta?\",\n",
    "]\n",
    "\n",
    "for query in queries:\n",
    "    print(f\"\\n{'='*70}\")\n",
    "    print(f\"Query: {query}\")\n",
    "    print(f\"{'='*70}\\n\")\n",
    "\n",
    "    result = await local_search_tool(query)\n",
    "    if \"error\" in result:\n",
    "        print(f\"Error: {result['error']}\")\n",
    "    else:\n",
    "        print(f\"Answer: {result['answer'][:500]}...\")\n",
    "        print(f\"\\nContext: {result['context']}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9",
   "metadata": {},
   "source": [
    "## 4. Test Global Search (Thematic)\n",
    "\n",
    "**Note:** Global search uses map-reduce across all communities, making many parallel LLM calls. May be slow or hit rate limits."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "10",
   "metadata": {},
   "outputs": [],
   "source": [
    "from mcp_server.tools.global_search import global_search_tool\n",
    "\n",
    "query = \"What are the main projects at TechVenture Inc?\"\n",
    "print(f\"Query: {query}\\n\")\n",
    "\n",
    "result = await global_search_tool(query)\n",
    "if \"error\" in result:\n",
    "    print(f\"Error: {result['error']}\")\n",
    "    if \"details\" in result:\n",
    "        print(f\"Details: {result['details']}\")\n",
    "else:\n",
    "    print(f\"Answer:\\n{result['answer']}\")\n",
    "    print(f\"\\nContext: {result['context']}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "11",
   "metadata": {},
   "source": [
    "## 5. Cross-Document Reasoning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "12",
   "metadata": {},
   "outputs": [],
   "source": [
    "complex_queries = [\n",
    "    \"Who are the customers of Project Beta and what incidents affected them?\",\n",
    "    \"What is the connection between David Kumar, Sophia Lee, and the GraphRAG incident?\",\n",
    "]\n",
    "\n",
    "for query in complex_queries:\n",
    "    print(f\"\\n{'='*70}\")\n",
    "    print(f\"Query: {query}\")\n",
    "    print(f\"{'='*70}\\n\")\n",
    "\n",
    "    result = await local_search_tool(query)\n",
    "    if \"error\" in result:\n",
    "        print(f\"Error: {result['error']}\")\n",
    "    else:\n",
    "        print(f\"Answer: {result['answer'][:500]}...\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "13",
   "metadata": {},
   "source": [
    "## Summary\n",
    "\n",
    "This notebook demonstrates:\n",
    "\n",
    "1. MCP Server connectivity check\n",
    "2. Entity query tool (list/search entities)\n",
    "3. Local search (entity-focused queries)\n",
    "4. Global search (thematic/community queries)\n",
    "5. Cross-document reasoning\n",
    "\n",
    "**Testing via HTTP:** Use [MCP Inspector](https://modelcontextprotocol.io/docs/tools/inspector) for interactive server testing:\n",
    "```bash\n",
    "npx @modelcontextprotocol/inspector\n",
    "```\n",
    "\n",
    "**Next Steps:**\n",
    "- Part 3: Supervisor Agent Pattern (agent-framework integration)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
